{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8mJdWYwPdVeBJbKmAD5Oi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarveshs1604/SRM/blob/main/ml_exp_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f582c29",
        "outputId": "7eaa4207-91cb-4573-e78d-f46a021ecca8"
      },
      "source": [
        "!pip install hmmlearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hmmlearn in /usr/local/lib/python3.12/dist-packages (0.3.3)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.12/dist-packages (from hmmlearn) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.12/dist-packages (from hmmlearn) (1.6.1)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.12/dist-packages (from hmmlearn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgQUQ3QVqo0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57988eb1-dce3-446b-80d3-b3bdf2fb69fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:hmmlearn.hmm:MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
            "https://github.com/hmmlearn/hmmlearn/issues/335\n",
            "https://github.com/hmmlearn/hmmlearn/issues/340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observed sequence: ['Walk', 'Shop', 'Clean', 'Shop', 'Walk']\n",
            "Most likely hidden states: ['Sunny', 'Sunny', 'Sunny', 'Sunny', 'Sunny']\n",
            "Log probability: -6.98143109837347\n"
          ]
        }
      ],
      "source": [
        "from hmmlearn import hmm\n",
        "import numpy as np\n",
        "\n",
        "# Hidden states and observation symbols\n",
        "states = [\"Sunny\", \"Rainy\"]\n",
        "observations = [\"Walk\", \"Shop\", \"Clean\"]\n",
        "\n",
        "# Define HMM parameters\n",
        "start_prob = np.array([0.7, 0.3])\n",
        "trans_mat = np.array([[0.8, 0.2],\n",
        "                     [0.3, 0.7]])\n",
        "emit_mat = np.array([[0.6, 0.3, 0.1],  # Sunny\n",
        "                    [0.1, 0.4, 0.5]])  # Rainy\n",
        "\n",
        "# Build the model\n",
        "model = hmm.MultinomialHMM(n_components=2, n_iter=100)\n",
        "model.startprob_ = start_prob\n",
        "model.transmat_ = trans_mat\n",
        "model.emissionprob_ = emit_mat\n",
        "model.n_trials = 1  # one trial per observation\n",
        "\n",
        "# Observed sequence: Walk(0), Shop(1), Clean(2), Shop(1), Walk(0)\n",
        "# Convert to one-hot representation (n_samples Ã— n_symbols)\n",
        "obs_seq = np.array([\n",
        "    [1, 0, 0],  # Walk\n",
        "    [0, 1, 0],  # Shop\n",
        "    [0, 0, 1],  # Clean\n",
        "    [0, 1, 0],  # Shop\n",
        "    [1, 0, 0],  # Walk\n",
        "])\n",
        "\n",
        "# Decode (Viterbi)\n",
        "logprob, hidden_states = model.decode(obs_seq, algorithm=\"viterbi\")\n",
        "\n",
        "print(\"Observed sequence:\", [observations[np.argmax(o)] for o in obs_seq])\n",
        "print(\"Most likely hidden states:\", [states[i] for i in hidden_states])\n",
        "print(\"Log probability:\", logprob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "179f0f76"
      },
      "source": [
        "**Aim:**\n",
        "\n",
        "The aim of this code is to demonstrate how to build and use a Hidden Markov Model (HMM) to infer the most likely sequence of hidden states (weather: Sunny or Rainy) given a sequence of observed events (activities: Walk, Shop, Clean).\n",
        "\n",
        "**Viterbi Algorithm Steps:**\n",
        "\n",
        "1. **Initialization:** Create a Viterbi path probability table and a backpointer table. Initialize the first column of the Viterbi table using the initial state probabilities and the emission probabilities of the first observation.\n",
        "2. **Recursion:** For each subsequent observation, iterate through each possible hidden state. Calculate the probability of reaching the current state from each previous state, considering the transition probabilities and the emission probability of the current observation.\n",
        "3. **Maximization:** For each current state, select the path from the previous states that has the maximum probability and store this maximum probability in the Viterbi table. Store the index of the previous state that yielded the maximum probability in the backpointer table.\n",
        "4. **Termination:** After processing all observations, find the hidden state in the last column of the Viterbi table that has the maximum probability. This state is the end of the most likely hidden state sequence.\n",
        "5. **Backtracking:** Starting from the final hidden state identified in the termination step, use the backpointer table to trace back the sequence of hidden states that led to this final state.\n",
        "6. **Sequence Reconstruction:** The sequence of hidden states obtained by backtracking is the most likely sequence of hidden states given the observed sequence.\n",
        "7. **Start:** Define the initial probabilities of being in each hidden state.\n",
        "8. **Process Observations:** Iterate through the observed sequence, applying the recursion and maximization steps for each observation.\n",
        "9. **End:** The algorithm concludes after backtracking and reconstructing the most likely hidden state sequence."
      ]
    }
  ]
}